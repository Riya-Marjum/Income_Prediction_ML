# -*- coding: utf-8 -*-
"""Income_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w57y1tGcLE8gTJIGs36vbSOZzBLX5Pm1

ðŸ“‚ Dataset Description

Adult Income Dataset

Goal: Predict whether income >50K or <=50K
"""

#data preprocessing practice

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
data=pd.read_csv("https://raw.githubusercontent.com/tech4alltraining/aiml/refs/heads/main/datasets/classification/adult.csv")

data

data.head()

data.tail()

data.shape  #no. of rows and coloumns(row,coloumn)

data.dtypes #datatype of each coloumn

data.describe()   #summary statistics of the data

data2=data.select_dtypes('object').columns.tolist()  #prints categorical coloumns
data2

data3=data.select_dtypes('int64','float64').columns.tolist()    #prints numerical coloumns
data3

#distinct value of each categorical feature

for col in data2:
  print(data[col].unique())

# visualize bar plot for any 3 categorical feature

categ = ['workclass', 'education', 'sex']
for col in categ:
    sns.countplot(x=data[col])
    plt.title(f'Distribution of {col}')
    plt.xticks(rotation=45)
    plt.show()

#Plot the correlation matrix (using heatmap) to explore relationships between numerical features

corr_matrix = data[data3].corr()

plt.figure(figsize=(10,8))# Plot heatmap

sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)    #annot=True(shows no inside the square)  #cmap(color map=shows which color scheme is used)
plt.title("Correlation Matrix of Numerical Features")                    #red for negative correlation,blue for positive correlation
plt.show()

#Examine the distribution of at least three numerical features*.
features=['age','capital.gain','hours.per.week']
plt.hist(data[features],color=['red','blue','yellow'])
plt.title(f'histogram of{features}')
plt.show()

#Data processing

data.replace('?',np.nan,inplace=True)

data

data.isna().sum()

# categorical features with missing values imputation is done using fillna()

data['workclass'].fillna(data['workclass'].mode()[0], inplace=True)
data['occupation'].fillna(data['occupation'].mode()[0], inplace=True)
data['native.country'].fillna(data['native.country'].mode()[0], inplace=True)
data.isna().sum()

#or
# Fill missing values in categorical columns with mode
for col in data2:
    data[col].fillna(data[col].mode()[0], inplace=True)
data.isnull().sum()

data.duplicated().sum()

#remove unwanted coloumn

data.drop(columns='fnlwgt',inplace=True)

data

#remove duplicated rows
data.duplicated().sum()

data.drop_duplicates(inplace=True)

data.duplicated().sum()

data

data3 = data.select_dtypes('int64', 'float64').columns.tolist()

# using boxplot to visualize the outliers present
for col in data3:
  sns.boxplot(data[col])
  plt.title('Boxplot of '+col)
  plt.figure(figsize=(10,5))
  plt.show()

# detect and remove outliers
for col in data3:
#Identify the quartiles
  q1,q3=np.percentile(data[col],[25,75])
#Calculate the interquartile range
  iqr=q3-q1
#Calculate the lower and upper bounds
  lower_bound=q1-(1.5*iqr)
  upper_bound=q3+(1.5*iqr)
  print(lower_bound, upper_bound)
# Drop the outliers
  data = data[(data[col] >= lower_bound)  & (data[col] <= upper_bound)]
data

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
le.fit(data['workclass'])  # Assuming 'workclass' is your column

print(le.classes_)        # Shows unique classes
print(le.transform(le.classes_))  # Shows label encoding

#label encoding which converts categorical features to numerical

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
for col in data2:
 data[col] = le.fit_transform(data[col])
data

data.dtypes    #Check if label encoding is done or not

data3

#scaling ,standardScaling is done

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
for col in data3:
  data[data3]=sc.fit_transform(data[data3])
data

data.head()

"""âœ…Preprocessing Completed

Dataset is now cleaned, encoded, scaled and ready for ML model building.

Model Building and Training
"""

# Train and Test
from sklearn.model_selection import train_test_split #function
# Define X,y
x = data.drop('income', axis=1)
y = data['income']
# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

print(data.shape)
print(x.shape)
print(y.shape)
#20% Testing 75, Training (80%): 300
print(x_train.shape) #300x1
print(y_train.shape) #300x1
print(x_test.shape)  #75x1
print(y_test.shape)  #75x1

# Model selection LogisticRegression
from sklearn.linear_model import LogisticRegression  #import class using scikit learn
lr = LogisticRegression(max_iter=1000)         #create object
lr.fit(x_train,y_train)         #Train model using data

# Model selection KNN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train,y_train)

# Model SVM
from sklearn.svm import SVC
svm = SVC(kernel='linear')
svm.fit(x_train,y_train)

# Model GNB
from sklearn.naive_bayes import GaussianNB
nb = GaussianNB()
nb.fit(x_train, y_train)

# Model Decision Tree
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(x_train, y_train)

# Model RandomForest
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=250)
rf.fit(x_train, y_train)

# Model MLP
from sklearn.neural_network import MLPClassifier
# Create an MLP classifier
mlp = MLPClassifier(hidden_layer_sizes=(10,10,5), max_iter=1000, random_state=42)

# Train the MLP model
mlp.fit(x_train, y_train)

# Model Gradient Boosting Classifier
from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier()
gb.fit(x_train, y_train)

# Model Extreme Gradient Boosting
from xgboost import XGBClassifier

xgb = XGBClassifier()
xgb.fit(x_train, y_train)

"""PREDICTION"""

y_test

y_pred_logreg = lr.predict(x_test)
y_pred_logreg

y_pred_knn = knn.predict(x_test)
y_pred_knn

y_pred_svm = svm.predict(x_test)
y_pred_svm

y_pred_nb = nb.predict(x_test)
y_pred_nb

y_pred_dt = dt.predict(x_test)
y_pred_dt

y_pred_rf = rf.predict(x_test)
y_pred_rf

y_pred_mlp = mlp.predict(x_test)
y_pred_mlp

y_pred_gb = gb.predict(x_test)
y_pred_gb

y_pred_xgb = xgb.predict(x_test)
y_pred_xgb

"""Model is Trained,Build and Prediction is doneâœ…

Model Evaluation ,Comparison and Conclusion :
"""

# Accuracy,Confusion Matrix and Classification report
from sklearn.metrics import accuracy_score,confusion_matrix, classification_report
print(accuracy_score(y_test, y_pred_logreg))
print(confusion_matrix(y_test, y_pred_logreg))
print(classification_report(y_test, y_pred_logreg))

# prompt: plot confusion matrix

import matplotlib.pyplot as plt
cm = confusion_matrix(y_test, y_pred_logreg)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Accuracy,Confusion Matrix and Classification report
from sklearn.metrics import accuracy_score,confusion_matrix, classification_report
print(accuracy_score(y_test, y_pred_knn))
print(confusion_matrix(y_test, y_pred_knn))
print(classification_report(y_test, y_pred_knn))

# prompt: plot confusion matrix

import matplotlib.pyplot as plt
cm = confusion_matrix(y_test, y_pred_knn)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Accuracy,Confusion Matrix and Classification report
from sklearn.metrics import accuracy_score,confusion_matrix, classification_report
print(accuracy_score(y_test, y_pred_svm))
print(confusion_matrix(y_test, y_pred_svm))
print(classification_report(y_test, y_pred_svm))

# prompt: plot confusion matrix

import matplotlib.pyplot as plt
cm = confusion_matrix(y_test, y_pred_svm)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Accuracy,Confusion Matrix and Classification Report
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
print(accuracy_score(y_test,y_pred_nb))
print(confusion_matrix(y_test,y_pred_nb))
print(classification_report(y_test,y_pred_nb))

# prompt: Confusion Matrix
import matplotlib.pyplot as plt
cm=confusion_matrix(y_test,y_pred_nb)
plt.figure(figsize=(8,6))
sns.heatmap(cm,annot=True,fmt='d',cmap= 'Blues',cbar= False,xticklabels=le.classes_,yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Accuracy,Confusion Matrix and Classification report
from sklearn.metrics import accuracy_score,confusion_matrix, classification_report
print(accuracy_score(y_test, y_pred_dt))
print(confusion_matrix(y_test, y_pred_dt))
print(classification_report(y_test, y_pred_dt))

import matplotlib.pyplot as plt
cm=confusion_matrix(y_test,y_pred_dt)
plt.figure(figsize=(8,6))
sns.heatmap(cm,annot=True,fmt='d',cmap= 'Blues',cbar= False,xticklabels=le.classes_,yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Accuracy,Confusion Matrix and Classification Report
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
print(accuracy_score(y_test,y_pred_rf))
print(confusion_matrix(y_test,y_pred_rf))
print(classification_report(y_test,y_pred_rf))

import matplotlib.pyplot as plt
cm=confusion_matrix(y_test,y_pred_rf)
plt.figure(figsize=(8,6))
sns.heatmap(cm,annot=True,fmt='d',cmap= 'Blues',cbar= False,xticklabels=le.classes_,yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Accuracy,Confusion Matrix and Classification Report
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
print(accuracy_score(y_test,y_pred_mlp))
print(confusion_matrix(y_test,y_pred_mlp))
print(classification_report(y_test,y_pred_mlp))

import matplotlib.pyplot as plt
cm=confusion_matrix(y_test,y_pred_mlp)
plt.figure(figsize=(8,6))
sns.heatmap(cm,annot=True,fmt='d',cmap= 'Blues',cbar= False,xticklabels=le.classes_,yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Accuracy,Confusion Matrix and Classification Report
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
print(accuracy_score(y_test,y_pred_gb))
print(confusion_matrix(y_test,y_pred_gb))
print(classification_report(y_test,y_pred_gb))

import matplotlib.pyplot as plt
cm=confusion_matrix(y_test,y_pred_gb)
plt.figure(figsize=(8,6))
sns.heatmap(cm,annot=True,fmt='d',cmap= 'Blues',cbar= False,xticklabels=le.classes_,yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Accuracy,Confusion Matrix and Classification Report
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
print(accuracy_score(y_test,y_pred_xgb))
print(confusion_matrix(y_test,y_pred_xgb))
print(classification_report(y_test,y_pred_xgb))

import matplotlib.pyplot as plt
cm=confusion_matrix(y_test,y_pred_xgb)
plt.figure(figsize=(8,6))
sns.heatmap(cm,annot=True,fmt='d',cmap= 'Blues',cbar= False,xticklabels=le.classes_,yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""HYPERPARAMETER TUNING

Hypertuning of knn
"""

k=[3,5,7,9]
for values in k:
 kn = KNeighborsClassifier(n_neighbors=values)
 kn.fit(x_train,y_train)
 y_pred=kn.predict(x_test)
# Accuracy,Confusion Matrix and Classification Report
 print(accuracy_score(y_test,y_pred))
 print(classification_report(y_test,y_pred))

"""k=7 has more accuracy so it is selected as best k

HyperTuning of SVM
"""

kernel=['linear','rbf','poly']
for k in kernel:
  sm =SVC(kernel=k,class_weight='balanced')
  sm.fit(x_train,y_train)
  y_pred=sm.predict(x_test)
  # Accuracy,Confusion Matrix and Classification Report
  print(accuracy_score(y_test,y_pred))
  print(classification_report(y_test,y_pred))

"""Here kernel=poly is better .That is here class 1 was not recognized, all 754 class 1 were mismatched so we balanced the datasets.

HyperTuning of RandomForest
"""

n_est=[50,100,150]
for val in n_est:
 raf = RandomForestClassifier(n_estimators=val, random_state=42)
 raf.fit(x_train, y_train)
 y_pred=raf.predict(x_test)
 # Accuracy,Confusion Matrix and Classification Report
 print(accuracy_score(y_test,y_pred))
 print(classification_report(y_test,y_pred))

"""Among the tested values, n_estimators=100 gave the best overall balance of accuracy and minority class detection, making it the preferred choice for final model deployment.

| **Model**              | **Hyperparameter(s)**       | **Accuracy (%)** | **Precision** | **Recall** | **F1-Score** |
| ---------------------- | --------------------------- | ---------------- | ------------- | ---------- | ------------ |
| Logistic Regression    | â€”                           | 81.5             | 0.79          | 0.82       | 0.79         |
| kNN                    |k=3                          | 80.2             | 0.80          |0.80        |0.80          |          
| kNN                    | k = 5                       | 81.6             | 0.81          | 0.82       | 0.81         |
| kNN                    | k = 7                       | 82.35            | 0.81          | 0.82       | 0.82         |
| kNN                    | k = 9                       | 82.32            | 0.81          | 0.82       | 0.82         |
| SVM                    | kernel = 'linear'           | 70.9             | 0.82          | 0.71       | 0.74         |
| SVM                    | kernel = 'rbf'              | 70.2             | 0.82          | 0.70       | 0.73         |
| SVM                    | kernel = 'poly'             |72.5              | 0.83          | 0.73       |0.75          |
| Decision Tree          | ---          | 76.3         | 0.76             | 0.76          | 0.76       |
| Random Forest          | n_estimators = 50           | 80.8             | 0.80          | 0.81       | 0.80         |
| Random Forest          | n_estimators = 100          | 81.0             | 0.80          | 0.81       | 0.80         |
| Random Forest          | n_estimators = 150          | 80.8             | 0.80          | 0.81       | 0.80         |
| Naive Bayes (Gaussian) | â€”                           | 79.1             | 0.82          | 0.79       | 0.80         |
| MLP Classifier         | ---          | 83.0         | 0.81             | 0.83          | 0.81       |
| Gradient Boosting      | ---          | 83.8         | 0.83             | 0.84          | 0.83       |
| XGBoost                | ---          | 83.5         | 0.83             | 0.84          | 0.83       |

Multiple machine learning models were evaluated for income classification using metrics such as Accuracy, Precision, Recall, and F1-Score.

The dataset had non-linear patterns and slight class imbalance, requiring models that balance accuracy and generalization.

Best Models:
Gradient Boosting and XGBoost delivered the highest performance across all metrics:

Accuracy: 83.5-83.8%

F1-Score: 0.83 (well-balanced across classes)

MLP Classifier also showed strong generalization with:

Accuracy: 83.0%

F1-Score: 0.81

Moderate Performers:
k-Nearest Neighbors (k=7, 9):
Reliable with F1-Score: 0.82; simple but effective baseline model.

Logistic Regression:
Competitive accuracy (81.5%) with good interpretability.

Underperformers:
Support Vector Machines (SVM):
Lower accuracy (~70 -72%) and poor recall due to possible feature scaling issues or class imbalance.

Decision Tree:
Prone to overfitting; accuracy limited to ~76%.


Conclusion:
Gradient Boosting and XGBoost are recommended as the final models due to their high accuracy, balanced precision-recall, and robustness across different performance metrics. They are most suitable for deployment in this income classification ta

In this project, multiple machine learning models were built, trained, and evaluated to predict whether an individual earns more than 50K per year based on demographic and employment-related features.

Through extensive experimentation and comparative analysis, Gradient Boosting and XGBoost emerged as the most effective models, achieving high accuracy and balanced classification performance.

These models demonstrated the ability to generalize well, especially in handling class imbalance and non-linear relationships within the data, making them highly suitable for real-world deployment in income classification systems.âœ…
"""

!pip install gradio

!pip install gradio --quiet

import gradio as gr
import numpy as np
import joblib  # or pickle
from sklearn.preprocessing import StandardScaler # Import StandardScaler

# Label encoding map for 'workclass'
workclass_mapping = {
    'Federal-gov': 0,
    'Local-gov': 1,
    'Never-worked': 2,
    'Private': 3,
    'Self-emp-inc': 4,
    'Self-emp-not-inc': 5,
    'State-gov': 6,
    'Without-pay': 7
}

# Define the numerical features used for scaling
numerical_features = ['age', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']


scaler = StandardScaler()
# Fit the scaler only on the numerical features from the data DataFrame
scaler.fit(data[numerical_features])
model = xgb


# Prediction function
def predict_income(age, education_num, capital_gain, capital_loss, hours_per_week, workclass):
    try:
        # Encode workclass
        workclass_encoded = workclass_mapping.get(workclass, -1)
        if workclass_encoded == -1:
            return "âŒ Invalid workclass selected."

        # Prepare input data as a dictionary with correct column names
        input_data_dict = {
            'age': age,
            'education.num': education_num,
            'capital.gain': capital_gain,
            'capital.loss': capital_loss,
            'hours.per.week': hours_per_week,
            'workclass': workclass_encoded,
            # Add other features with a default or appropriate value, matching the training data columns
            'education': 0, # Example: provide a default or representative encoded value
            'marital.status': 0, # Example
            'occupation': 0, # Example
            'relationship': 0, # Example
            'race': 0, # Example
            'sex': 0, # Example
            'native.country': 0 # Example
        }

        # Convert to DataFrame to ensure consistent column order and structure
        input_df = pd.DataFrame([input_data_dict])

        # Select and scale numerical features
        input_scaled_numerical = scaler.transform(input_df[numerical_features])



        input_for_prediction = np.array([[
            input_scaled_numerical[0][0], # age (scaled)
            workclass_encoded,            # workclass (encoded)
            input_df['education'].iloc[0], # education (encoded - using default)
            input_scaled_numerical[0][1], # education.num (scaled)
            input_df['marital.status'].iloc[0], # marital.status (encoded - using default)
            input_df['occupation'].iloc[0], # occupation (encoded - using default)
            input_df['relationship'].iloc[0], # relationship (encoded - using default)
            input_df['race'].iloc[0], # race (encoded - using default)
            input_df['sex'].iloc[0], # sex (encoded - using default)
            input_scaled_numerical[0][2], # capital.gain (scaled)
            input_scaled_numerical[0][3], # capital.loss (scaled)
            input_scaled_numerical[0][4], # hours.per.week (scaled)
            input_df['native.country'].iloc[0] # native.country (encoded - using default)
        ]])


        # Predict using your trained model
        prediction = model.predict(input_for_prediction)

        return "âœ… Income >50K" if prediction[0] == 1 else "âœ… Income <=50K"

    except Exception as e:
        return f"âŒ Error: {str(e)}"

# Create Gradio UI
interface = gr.Interface(
    fn=predict_income,
    inputs=[
        gr.Number(label="Age"),
        gr.Number(label="Education Number (1â€“16)"),
        gr.Number(label="Capital Gain"),
        gr.Number(label="Capital Loss"),
        gr.Number(label="Hours per Week"),
        gr.Dropdown(
            list(workclass_mapping.keys()),
            label="Workclass"
        )
    ],
    outputs=gr.Text(label="Prediction"),
    title="Income Prediction App",
    description="Enter your details to predict whether income is >50K or <=50K."
)

# Launch the Gradio interface
interface.launch(share=True)

